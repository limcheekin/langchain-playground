{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTranslate2\n",
    "\n",
    "## Overview\n",
    "\n",
    "CTranslate2 is a C++ and Python library for efficient inference with Transformer models. The project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to accelerate and reduce the memory usage of Transformer models on CPU and GPU.\n",
    "\n",
    "I try to use it for Flan-T5 models by refer to the sample codes from https://opennmt.net/CTranslate2/guides/transformers.html#t5\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU ctranslate2 transformers[torch] sentencepiece"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model\n",
    "### Use Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ctranslate2 \n",
    "#model_id = \"google/flan-t5-small\"\n",
    "#ct = ctranslate2.converters.TransformersConverter(model_name_or_path=model_id)\n",
    "#ct.convert(output_dir=\"google/flan-t5-small-ct2\", force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ct2-transformers-converter --model google/flan-t5-small --output_dir google/flan-t5-small-ct2 --quantization int8 --force\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctranslate2\n",
    "import transformers\n",
    "\n",
    "translator = ctranslate2.Translator(\"google/flan-t5-xl-ct2\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/flan-t5-xl-ct2\")\n",
    "\n",
    "input_text = \"translate English to German: The house is wonderful.\"\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "results = translator.translate_batch([input_tokens])\n",
    "\n",
    "output_tokens = results[0].hypotheses[0]\n",
    "output_text = tokenizer.decode(tokenizer.convert_tokens_to_ids(output_tokens))\n",
    "\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is AI? Tell me more about it.\"\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "results = translator.generate_tokens(input_tokens)\n",
    "\n",
    "END_TOKEN = '</s>'\n",
    "for i, result in enumerate(results):\n",
    "    if result.token != END_TOKEN:\n",
    "        if i == 0:\n",
    "            print(result.token.replace('▁', ''), end='')\n",
    "        else:    \n",
    "            print(result.token.replace('▁', ' '), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What is AI? Tell me more about it.\"\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))\n",
    "\n",
    "results = translator.translate_iterable([input_tokens])\n",
    "\n",
    "for result in results:\n",
    "    for hypothesis in result.hypotheses:\n",
    "        print(tokenizer.decode(tokenizer.convert_tokens_to_ids(hypothesis)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.llms.ctranslate2 import Ct2Translator\n",
    "llm = Ct2Translator(model_path=\"../ct2/ct2fast-flan-alpaca-xl\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "llm_chain.run(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
