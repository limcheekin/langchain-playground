{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1dUMCB_kiTom"
      },
      "source": [
        "# MPT-30B-Instruct\n",
        "### Setup the prompt & LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.llms import Modal\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import os\n",
        "\n",
        "template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n###Instruction\\n{instruction}\\n\\n### Response\\n\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"instruction\"])\n",
        "url = os.environ.get(\"MPT_30B_INSTRUCT_URL\") \n",
        "llm = Modal(endpoint_url=url, include_prompt_in_result=False)\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
        "\n",
        "\"\"\" params\n",
        "config.max_seq_len = 16384\n",
        "pipe(prompt,\n",
        "     max_new_tokens=256,\n",
        "     do_sample=True,\n",
        "     temperature=0.7,\n",
        "     top_p =0.95,\n",
        "     top_k =  50,\n",
        "     eos_token_id = 0,\n",
        "     use_cache=True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qHrby3n0109",
        "outputId": "1b7b7f12-ce3d-4502-db84-2c423d6aebcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "What are the differences between alpacas, vicunas and llamas?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "What are the differences between alpacas, vicunas and llamas?\n",
            "\n",
            "### Response\n",
            "Alpacas, vicunas and llamas are all types of South American camelids. They are all used for their fleece, as well as for meat and as pack animals.\n",
            "\n",
            "Alpacas are a domesticated species, and are typically between 3.5 and 4.5 feet tall. They have a soft, dense fleece that is typically used in high-end clothing.\n",
            "\n",
            "Vicunas are a wild species, and are typically between 4 and 5 feet tall. Their fleece is also soft and dense, but typically has more guard hair and is therefore typically used for lower-end clothing.\n",
            "\n",
            "Llamas are a domesticated species, and are typically between 5 and 6 feet tall. They have a coarser fleece that is typically used for rugs, blankets and other items.\n",
            "CPU times: user 25.1 ms, sys: 5.94 ms, total: 31.1 ms\n",
            "Wall time: 2min 25s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'What are the differences between alpacas, vicunas and llamas?'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "angnwW9HG4Hv",
        "outputId": "7bd55a80-23fe-4dd4-f634-2260d85330af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "What is the capital of England?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "What is the capital of England?\n",
            "\n",
            "### Response\n",
            "London is the capital of England\n",
            "CPU times: user 30.3 ms, sys: 977 µs, total: 31.2 ms\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'What is the capital of England?'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGCCFto2G4Jk",
        "outputId": "35ab7e65-77ee-44a2-fc4f-afa0c6c456a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Write an email to Sam Altman giving reasons to open source GPT-4\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Write an email to Sam Altman giving reasons to open source GPT-4\n",
            "\n",
            "### Response\n",
            "Hi Sam,\n",
            "\n",
            "I hope this email finds you well. I am a big admirer of OpenAI and the incredible work you and the team are doing to advance the state of AI. I am writing to you today to express my support for open sourcing GPT-4 and share the reasons that I believe this is the right thing to do. \n",
            "\n",
            "Firstly, open sourcing GPT-4 will enable the broader research community to contribute to and advance the model. This will lead to faster innovation and improvement of the model, which will ultimately benefit OpenAI and the broader community. \n",
            "\n",
            "Secondly, open sourcing GPT-4 will increase transparency in the field of AI and help to build trust and confidence in OpenAI and the work that you do. This is critical to the long term success of OpenAI and the field of AI more broadly. \n",
            "\n",
            "Finally, open sourcing GPT-4 will help to build goodwill and trust with the broader community, which will be beneficial to OpenAI in the long run. \n",
            "\n",
            "I sincerely hope that you will consider open sourcing GPT-4 and I look forward to seeing what the future brings for OpenAI and the field of AI. \n",
            "\n",
            "Best regards, \n",
            "\n",
            "Your supporter, \n",
            "\n",
            "[Name]\n",
            "CPU times: user 26.9 ms, sys: 4.84 ms, total: 31.7 ms\n",
            "Wall time: 3min 44s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Write an email to Sam Altman giving reasons to open source GPT-4'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9uswqYmG4LZ",
        "outputId": "4857cf4f-c9d9-43eb-9728-26e53dfbec9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "As an AI do you like the Simpsons? What do you know about Homer?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "As an AI do you like the Simpsons? What do you know about Homer?\n",
            "\n",
            "### Response\n",
            "I have processed all available Simpsons content and have a very detailed understanding of the show and its characters. Homer is the father of the Simpson family and is generally portrayed as an incompetent, lazy, and immature man. He is married to Marge and has three children, Bart, Lisa, and Maggie. He is the owner of the local power plant and is a generally well-meaning but flawed character.\n",
            "CPU times: user 17.1 ms, sys: 7.4 ms, total: 24.5 ms\n",
            "Wall time: 1min 23s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'As an AI do you like the Simpsons? What do you know about Homer?'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYM0_ryUG4NO",
        "outputId": "3472e2e8-526d-4e8f-8b29-d153292faff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Tell me about Homer on the TV show the simpsons\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Tell me about Homer on the TV show the simpsons\n",
            "\n",
            "### Response\n",
            "Homer Simpson is the father on the TV show the simpsons.  He is married to Marge and they have three children, Bart, Lisa, and Maggie.  Homer works at the Springfield Nuclear Power Plant as a safety inspector.  He is known for his love of food and beer and is not very intelligent.  He is the opposite of his wife Marge who is very smart and intelligent.  Homer is the main character on the show and has appeared in every episode.\n",
            "CPU times: user 20.4 ms, sys: 2.04 ms, total: 22.4 ms\n",
            "Wall time: 48.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Homer on the TV show the simpsons'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFQ_jT0iMc4O",
        "outputId": "82974bb6-b69e-4967-d293-1ebd8b20f390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Tell me about Homer on the TV show the simpsons in depth\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Tell me about Homer on the TV show the simpsons in depth\n",
            "\n",
            "### Response\n",
            "Homer Simpson is the father on the show the simpsons.  He is a middle aged man who enjoys eating donuts and drinking beer.  He works at the nuclear power plant and is married to Marge and they have three children Bart, Lisa, and Maggie.  He is not the smartest person and often gets himself into trouble.  He is a fun character and has many funny moments on the show.\n",
            "CPU times: user 21 ms, sys: 0 ns, total: 21 ms\n",
            "Wall time: 42 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Homer on the TV show the simpsons in depth'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmbDQ82vMPYy",
        "outputId": "141822f7-afb6-458a-f064-5e025dd6fdec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
            "\n",
            "### Response\n",
            "Before buying 6 more apples, the cafeteria had 23 - 20 = 3 apples left. After buying 6 more apples, the cafeteria has 3 + 6 = 9 apples.\n",
            "CPU times: user 27.3 ms, sys: 0 ns, total: 27.3 ms\n",
            "Wall time: 45.1 s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "prompt = 'Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHKCo6VXNByX",
        "outputId": "1f2c9751-25c6-4519-b038-1d4fd0c905e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Answer the following yes\\/no question by reasoning step-by-step. \n",
            " Can you write a whole Haiku in a single tweet?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Answer the following yes\\/no question by reasoning step-by-step. \n",
            " Can you write a whole Haiku in a single tweet?\n",
            "\n",
            "### Response\n",
            "Yes, you can write a whole Haiku in a single tweet. A Haiku is a Japanese poem consisting of three lines with 17 syllables. The first line has 5 syllables, the second line has 7 syllables, and the third line has 5 syllables. Since a tweet has a character limit of 280, you can fit the 17 syllables in a single tweet if you use proper punctuation and spacing. Here is an example:\n",
            "\n",
            "First line of the Haiku: Spring - is - beautiful\n",
            "Second line of the Haiku: Cherry - blossoms - fill - the - air\n",
            "Third line of the Haiku: I - long - for - Japan\n",
            "CPU times: user 28.3 ms, sys: 462 µs, total: 28.8 ms\n",
            "Wall time: 1min 6s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Answer the following yes\\/no question by reasoning step-by-step. \\n Can you write a whole Haiku in a single tweet?'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsn1buh6NTie",
        "outputId": "c1ebd024-66fd-4e0c-cc75-fff7306ffb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Tell me about Harry Potter and studying at Hogwarts?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Tell me about Harry Potter and studying at Hogwarts?\n",
            "\n",
            "### Response\n",
            "Harry Potter is the main character in a series of seven books written by British author J. K. Rowling. The books concern a wizard called Harry Potter and his journey through Hogwarts School of Witchcraft and Wizardry. The stories tell of him overcoming dangerous obstacles to defeat the Dark Wizard Lord Voldemort, who killed his parents when Harry was 15 months old.\n",
            "\n",
            "The books have gained a wide adult readership as well as a large teenage following. They have been the subject of both praise and criticism from teachers and religious groups. Rowling has said that the series' central theme is death. The books have also inspired a series of films, video games, and theme park attractions.\n",
            "\n",
            "The first book in the series, Harry Potter and the Philosopher's Stone, was published in 1997 by Bloomsbury. It was the fastest-selling book of the decade, with initial print runs of 500,000 copies. The books have been translated into 67 languages, and as of March 2008, they had sold more than 200 million copies. The last book in the series, Harry Potter and the Deathly Hallows, was released on 21 July 2007.\n",
            "\n",
            "The books have won a number of awards, including the British Book Awards Children's Book of the Year (1999, for Harry Potter and the Chamber of Secrets), the Whitbread Book of the Year (1999, for Harry Potter and the Prisoner of Azkaban), and the Nestlé Smarties Book Prize (both 2002, for Harry Potter and the Goblet of Fire, and 2005, for Harry Potter and the Order of the Phoenix). They have also been the subject of some criticism from religious groups and teachers. Rowling has said that the series' central theme is death.\n",
            "\n",
            "The first film in the series, Harry Potter and the Philosopher's Stone, was released in 2001 by Warner Bros. and was a critical and commercial success. It was followed by seven more films, with the last two being split into two parts. The final film, Harry Potter and the Deathly Hallows – Part 2, was released on 15 July 2011. The films have been a financial success, with the first five films each taking in more than $934 million worldwide. The last two films each took in more than $1.34 billion worldwide\n",
            "CPU times: user 31.5 ms, sys: 403 µs, total: 31.9 ms\n",
            "Wall time: 4min 28s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell me about Harry Potter and studying at Hogwarts?'\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7qsYoDUT57D",
        "outputId": "bff3ee51-60da-45d0-80d7-5cdf58950499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Convert the following to JSON\n",
            "\n",
            "name: John\n",
            "age: 30\n",
            "address:\n",
            "street: 123 Main Street\n",
            "city: San Fransisco\n",
            "state: CA\n",
            "zip: 94101\n",
            "\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Convert the following to JSON\n",
            "\n",
            "name: John\n",
            "age: 30\n",
            "address:\n",
            "street: 123 Main Street\n",
            "city: San Fransisco\n",
            "state: CA\n",
            "zip: 94101\n",
            "\n",
            "\n",
            "### Response\n",
            "{\n",
            "  \"name\": \"John\",\n",
            "  \"age\": 30,\n",
            "  \"address\": {\n",
            "    \"street\": \"123 Main Street\",\n",
            "    \"city\": \"San Fransisco\",\n",
            "    \"state\": \"CA\",\n",
            "    \"zip\": \"94101\"\n",
            "  }\n",
            "}\n",
            "CPU times: user 23 ms, sys: 2.55 ms, total: 25.5 ms\n",
            "Wall time: 1min 39s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"\"\"Convert the following to JSON\n",
        "\n",
        "name: John\n",
        "age: 30\n",
        "address:\n",
        "street: 123 Main Street\n",
        "city: San Fransisco\n",
        "state: CA\n",
        "zip: 94101\n",
        "\"\"\"\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp_lI2mkrIls",
        "outputId": "a8f6be94-61e5-47e8-8db6-c9d43b8c4bc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "How are you today?\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "How are you today?\n",
            "\n",
            "### Response\n",
            "I'm doing well, thanks for asking!\n",
            "CPU times: user 21.2 ms, sys: 3.2 ms, total: 24.4 ms\n",
            "Wall time: 17.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"\"\"How are you today?\"\"\"\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfvXtlq1rNu9",
        "outputId": "73f866cb-4def-47e6-88f3-f522634a36d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Write me a short plan for a 3 day trip to London\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Write me a short plan for a 3 day trip to London\n",
            "\n",
            "### Response\n",
            "Day 1:\n",
            "Start your day with a delicious English breakfast. After breakfast, take the tube to Westminster to visit Big Ben and Parliament. In the afternoon, walk over to Buckingham Palace to watch the changing of the guard. End your day with a delicious fish and chips dinner.\n",
            "\n",
            "Day 2:\n",
            "Start your day with a traditional English breakfast. After breakfast, take a walk in Hyde Park and visit the Princess Diana Memorial Fountain. In the afternoon, visit the Natural History Museum and the Victoria and Albert Museum. End your day with a delicious Indian dinner.\n",
            "\n",
            "Day 3:\n",
            "Start your day with a traditional English breakfast. After breakfast, take a walk in Regent's Park and visit the London Zoo. In the afternoon, visit the British Museum. End your day with a delicious Italian dinner.\n",
            "CPU times: user 21.6 ms, sys: 6.88 ms, total: 28.5 ms\n",
            "Wall time: 1min 20s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"\"\"Write me a short plan for a 3 day trip to London\"\"\"\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yxtk83DVyR48"
      },
      "outputs": [],
      "source": [
        "article = \"\"\"\n",
        "Content moderators under Sama, Meta’s content review sub-contractor in Africa, earlier today picketed at the company’s headquarters in Kenya demanding April salary, while urging it to observe the court orders that barred it from conducting mass layoffs.\n",
        "\n",
        "The demonstrations came after Sama, in an email, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
        "\n",
        "The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
        "\n",
        "\n",
        "The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
        "\n",
        "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through our regular clearance process. This clearance process involves the return of company equipment to make sure that all final dues can be paid without deduction for that equipment, in accordance with Kenyan law.”\n",
        "\n",
        "It said the moderators’ contracts had ended in March after its deal with Meta expired, saying that it was only processing the moderators final dues.\n",
        "\n",
        "“We understand our former employees’ frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CEQDmM4xL-J",
        "outputId": "4e9cdabe-d582-4274-aa96-f7af0afd11a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Please summarize this article:\n",
            "\n",
            "Content moderators under Sama, Meta’s content review sub-contractor in Africa, earlier today picketed at the company’s headquarters in Kenya demanding April salary, while urging it to observe the court orders that barred it from conducting mass layoffs.\n",
            "\n",
            "The demonstrations came after Sama, in an email, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
            "\n",
            "The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
            "\n",
            "\n",
            "The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
            "\n",
            "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through our regular clearance process. This clearance process involves the return of company equipment to make sure that all final dues can be paid without deduction for that equipment, in accordance with Kenyan law.”\n",
            "\n",
            "It said the moderators’ contracts had ended in March after its deal with Meta expired, saying that it was only processing the moderators final dues.\n",
            "\n",
            "“We understand our former employees’ frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
            "\n",
            "\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Please summarize this article:\n",
            "\n",
            "Content moderators under Sama, Meta’s content review sub-contractor in Africa, earlier today picketed at the company’s headquarters in Kenya demanding April salary, while urging it to observe the court orders that barred it from conducting mass layoffs.\n",
            "\n",
            "The demonstrations came after Sama, in an email, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
            "\n",
            "The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
            "\n",
            "\n",
            "The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
            "\n",
            "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through our regular clearance process. This clearance process involves the return of company equipment to make sure that all final dues can be paid without deduction for that equipment, in accordance with Kenyan law.”\n",
            "\n",
            "It said the moderators’ contracts had ended in March after its deal with Meta expired, saying that it was only processing the moderators final dues.\n",
            "\n",
            "“We understand our former employees’ frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
            "\n",
            "\n",
            "\n",
            "### Response\n",
            "Sama, Meta’s content review sub-contractor in Africa, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders. The moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March. The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy. Sama was directed to continue reviewing content on Meta’s platforms, and to\n",
            "CPU times: user 40.7 ms, sys: 0 ns, total: 40.7 ms\n",
            "Wall time: 8min 16s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"Please summarize this article:\\n\" + article\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzko3r6mxU9-",
        "outputId": "e3a5ace3-5c19-4a1e-b2f7-1d55da8cf667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "###Instruction\n",
            "Please extract the key info as bullet points for this article:\n",
            "\n",
            "Content moderators under Sama, Meta’s content review sub-contractor in Africa, earlier today picketed at the company’s headquarters in Kenya demanding April salary, while urging it to observe the court orders that barred it from conducting mass layoffs.\n",
            "\n",
            "The demonstrations came after Sama, in an email, instructed moderators to clear with the company by May 11, a move the employees say is against the existing court orders.\n",
            "\n",
            "The 184 moderators sued Sama for allegedly laying them off unlawfully, after it wound down its content review arm in March, and Majorel, the social media giant’s new partner in Africa, for blacklisting on instruction by Meta.\n",
            "\n",
            "\n",
            "The court issued a temporary injunction on March 21 barring Sama from effecting any form of redundancy, and Meta from engaging Majorel, which was also instructed to refrain from blacklisting the moderators. Sama was directed to continue reviewing content on Meta’s platforms, and to be its sole provider in Africa pending determination of the case. However, Sama sent the moderators on compulsory leave in April saying it had no work for them as its contract with Meta had expired.\n",
            "\n",
            "Sama told TechCrunch that it had sent the notice “to staff whose contract had expired to go through our regular clearance process. This clearance process involves the return of company equipment to make sure that all final dues can be paid without deduction for that equipment, in accordance with Kenyan law.”\n",
            "\n",
            "It said the moderators’ contracts had ended in March after its deal with Meta expired, saying that it was only processing the moderators final dues.\n",
            "\n",
            "“We understand our former employees’ frustration because they were led by others to believe that they would all receive salary indefinitely while on leave, but that is not what the court dictated,” said Sama.\n",
            "\n",
            "\n",
            "\n",
            "### Response\n",
            "\u001b[0m\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m<timed exec>:2\u001b[0m\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/chains/base.py:256\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    255\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    259\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/chains/base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    146\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    147\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    149\u001b[0m )\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/chains/base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    133\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    135\u001b[0m     inputs,\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 139\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    140\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    141\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/chains/llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m     80\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m )\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/llms/base.py:135\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    131\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    134\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/llms/base.py:192\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    191\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    193\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/llms/base.py:186\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    181\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    182\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, invocation_params\u001b[39m=\u001b[39mparams\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    184\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 186\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    187\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    188\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    191\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/llms/base.py:449\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    446\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    448\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 449\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    450\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    451\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    452\u001b[0m     )\n\u001b[1;32m    453\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    454\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/langchain/llms/modal.py:83\u001b[0m, in \u001b[0;36mModal._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call to Modal endpoint.\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m---> 83\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mpost(\n\u001b[1;32m     84\u001b[0m     url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_url,\n\u001b[1;32m     85\u001b[0m     headers\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     86\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mContent-Type\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mapplication/json\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     87\u001b[0m     },\n\u001b[1;32m     88\u001b[0m     json\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams},\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mjson():\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(url, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, json\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, data\u001b[39m=\u001b[39;49mdata, json\u001b[39m=\u001b[39;49mjson, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/urllib3/connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "prompt = \"Please extract the key info as bullet points for this article:\\n\" + article\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K71mYsBx1r4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "transcript = \"\"\"\\\n",
        "How to Build a Domain Expert Chatbot Using Langchain & MindsDB | Harrison Chase & Patricio Mardini - YouTube\n",
        "https://www.youtube.com/watch?v=QrIEoexbtDo\n",
        "\n",
        "Transcript:\n",
        "(00:00) [Music] foreign [Music] before we go into that I I figured I would give a quick talk on link chain what it is how it relates to minusdb in particular interacting with data that that you might have in an urban price setting which which Minds to be can help you connect to so link chain is an open source developer framework for building applications with language models specifically we're focused on a lot of the glue that connects language models to other sources of data and computation so part of the thesis of laying chain is\n",
        "(00:50) that language models are are awesome and Powerful by themselves but they can be a lot more powerful a lot more personalized when they're connected to your particular data or your particular apis and help build more interesting applications that way we have Python and typescript packages really focused on composition and modularity so part of our belief also is that it's extremely early in this in this ecosystem it's extremely early in building these applications and so we want to focus on building a lot of the core components\n",
        "(01:16) that we think will be important we don't exactly know all the ways that they'll be put together to build applications but we want to focus on enabling all of those as best we can and so talking a little bit more about that we have a bunch of different components and then a bunch of ways of like stacking these together to have like more use case specific chains so so talking about the components there are two separate categories some are ones that we've implemented ourselves within langchain and then others are ones that\n",
        "(01:45) we just have Integrations for you know part of the exciting part about building in in this area is there's so much stuff going on there there are so many different model providers Vector store providers retrieval mechanisms that that we want to be a way to easily connect with all of those so those are the ones on the right those are models Vector stores retrievers we we don't actually have any language models of our own we just provide a standardized interface for interacting with with openai anthropic\n",
        "(02:10) cohere the open source models Mosaic everything like that so part of the in same thing for Vector stores we integrate with you know pine cone weevie we don't have our own and then same thing with more generic retrievers and the idea there is basically um you know I think there's a lot of value in being able to ease switch between like open AI or anthropic or a hugging face model and so having a standardized interface allows people to easily do that the other ones on the left are things that we've built into the core library\n",
        "(02:39) and so these are things that are new and we think are kind of like fundamental building blocks for building with language models um and I'm going to skim over these really quickly but there's uh documentation on our site about all those prompts are basically ways for interacting with language models these are a new thing right these didn't really exist you know up until a year or a few years ago and so we've got a lot of functionalities for creating and working with and composing different types of prompts output parsers is\n",
        "(03:07) basically taking the output of a language model which is a string and turning it into some structured information which is good when you need to do Downstream tasks with it example selectors are are ways of inserting few shot examples as a technique to construct more um comprehensive prompts document loaders are really important so their way of loading unstructured data so previously you know there there was some use cases for like web pages and PDFs but now there's a lot more because you can just kind of feed them into a\n",
        "(03:35) language model and ask questions about it so there's a lot of Integrations there text Splitters are also very important here a lot of language models have context length limits and so you need to split up text before passing to it and so we've got some Integrations there and then memory is a concept when you've got a chat bot and you're interacting with it it's stateful right it remembers what you said before and so that's a very very simple form of like short-term memory and you can start to\n",
        "(03:59) imagine more complex forms where where it remembers things over time and so so we've added some functionality there the things on the right are basically a bunch of applications that you can build with link chain and again these are just kind of like the the starting point we really think there are like they're and and and and so there are a lot of much more applications than these that you can build with Lane chains these are the ones that we have kind of like good support for just getting started with\n",
        "(04:22) like you know one two three lines off the shelf um and uh the goal is the goal between these two things is to make it really easy to get started with an end-to-end application basically but then also really easy to customize it for your own particular application by taking the modular parts um so because uh this is a minusdb conference I figured I would talk a little bit about how to interact with tabular Data before handing it off to Patricio and so when thinking about interacting with tabular data there's a\n",
        "(04:55) few different ways that you can do this and there's not one way that's right or wrong it just it depends on the type of data so the high level approaches as I kind of see them breaking out are one you can start to index the data into a vector store and this is really good when you have a lot of Text data that that is in a database so if you have one or two fields that are very text Heavy long text strings you're probably going to want to do things like interact with that do question answering over it and\n",
        "(05:21) so putting that in a vector store and enabling things like semantic search will be a lot more powerful on the other hand you can often have tabular data that's just numbers and so it's actually much more powerful to interact with that data directly whether it's strictly whether it's in the in a SQL database in the pandas data frame in a CSV and and so you can actually just interact with that data directly rather than transform where where it lives and then the third approach is combining these and getting\n",
        "(05:45) The Best of Both Worlds so diving into this a little bit this is a an overview of what it generally means to load data into a vector store and so this Powers a lot of the retrieval augmented generation if people are familiar with that term which is basically the idea that when you want to ask a question of a PDF or of documents that something like chat GPT hasn't seen the general approach for doing that is to retrieve those documents and then pass those into the prompt and basically say answer this question only with the data that's\n",
        "(06:16) present and so the first thing that you need to do there is some pre-processing step or an indexing step where basically you load the the documents into an index where you can then uh query them and get only the most relevant documents because this this is useful when you've got so many documents you can't just pass them all into the language model directly so you want to find the most relevant documents so you want to put them in an index that easily enables that and the general workflow here is up there you've\n",
        "(06:42) load it as documents you chunk them into small chunks you create an embedding which is a numerical Vector for each document which allows really quick similarity lookup and then you put those in a vector store which is a database designed to make that look up really really fast then once it's in there you can do the query so you fetch relevant documents you pass the documents and specified metadata into a prompt and then there's a few different methods that lane chain supports for actually uh do combining\n",
        "(07:07) this with the language model and these depend on the number of documents that you have the the length of them and so on and I'm not going to go into too much detail about them here concretely what this looks like in in link chain and I have like URLs to all the code Snippets as well and I'll probably I'll post these slides somewhere afterwards so they're they're available um for tabular data there's a CSV loader so you can load the the the data into documents if you look at the documents\n",
        "(07:34) here you can see that there's the page content where we where basically what happens is team is like a header column Nationals is the value payrolls is another header column and this is now on another line and then you've got the number which is another value and then you've got a bunch of metadata so where it came from in the row and this this can be useful for more complex Downstream things um you can then index it so this is creating the vector store it's four lines of code plus a few import\n",
        "(08:02) statements um and and the different components here the text splitter the embedding model and the vector sure are all are all swappable and then this is what it looks like to query it and so you pass in a language model this is the open AI language model here you you pacify you you pass in a way for combining these documents and you pass in the Retriever and then you can start asking the questions the next method for interacting with tabular data is just interacting with the data directly and so this is good when there's a lot of numerical data in\n",
        "(08:33) it doesn't really make a lot of sense to do semantic search on numerical data so what you can do is you can just interact with it directly and the two ways that we support that in link chain are interacting it with it if it's in a SQL database or interacting with it as if it's in a pandas data frame and if you've got some data in a CSV you can just load it into either one of those and do that and so what that concretely means is you use the language model to write a SQL query or write a python command that\n",
        "(09:00) helps answer the question and then you run that command or query get back a result and then use that to answer the question and so there's a bunch of nuance here the main one being how do you construct The Prompt with context so you need to tell it what tables are present what um what columns are in those tables what the types of those columns are sometimes for the categorical columns like what the values are there and so there's a lot of complexity that actually goes into letting the language model construct a\n",
        "(09:26) SQL query correctly once you've constructed that prompt you can pass it to a language model it will return a query you you run it you get back a result and that summarizes and then you summarize it with a language model again um and so this is this is what it generally looks like for a simple um uh SQL chain and so in this chain you get the incoming question that question is then turned into select count star from album where artist ID equals three it returns a result and then it passes it to a language model which summarizes\n",
        "(10:00) it and this is using a chain and then link chain chains are generally referred to like predetermined sequences of steps that are run in a particular order this contrasts with agents so agents are more complex where basically if a chain is a predetermined sequence of steps an agent now decides uh what steps to do and in what order so concretely it will decide to take one step run it get a result back then decide to take another step run it get a result back and so this is uh more complicated so they don't always work uh as as well but the\n",
        "(10:37) pros and it's also more costly so it takes longer the pros are that it can handle edge cases a lot better so there's a lot of conditions where just that simple predetermined sequence of the steps even if you start to add in like if statements or or things like that can start to get a little bit unwieldy it can also handle errors so if a step in that chain like errors you're kind of you're kind of out of luck but with an agent you can actually pass the error back and ask it to kind of fix its\n",
        "(11:01) mistake and then it can do multi-hop reasoning more easily so if it needs to look up one thing and then look up another thing based on the result of the first thing and then maybe even look up a 30 thing based on that agents can do that and I think Patricia's demo will actually show off an agent later on and so uh we we have SQL um and we have SQL and and pandas agents as well in link chain and you can see here there's a lot more things on the screen which you know we won't dive into it but basically you should treat that\n",
        "(11:28) as there's there's a lot more things going on which again is both good and bad um so here uh and this is this is working on like a really big database it actually looks up the relevant tables and the relevant schema because we don't pass those in because there's not enough context uh there's not enough space in the context window too and then on the right um You can see some more complex things where it's uh where it's doing a few different queries and then the last point that I'll say is\n",
        "(11:54) that you can actually just combine these as well so you know there's powers to both of them there's powers of putting in a vector store and doing similarity search you can capture a lot of semantic meaning there but again if there's numbers um there that that might not be the best approach and again when combining them there's a few different ways you can do this there's a chain which is a more simpler thing which is basically picking one and then going to that one and then there's an agent which is picking one\n",
        "(12:18) going to one getting back a response picking another one going to that getting back another response and then exiting whenever it feels like it has the correct answer um and and and so those are both supported in link chain as well with and you can see here it's routing so can you see the mouse yeah so here you can see that it's routing to a particular Vector store which we've named like State of the Union or Paul Graham essay or personal um and then in the agent you can see that the action corresponds to uh the\n",
        "(12:47) the place where it needs to go so similar ideas this again is powerful because it's handling multi-hop it's actually it's it's looking up things from multiple different data sources in one go this is this is more simple so it's you know more controllable it's easy to understand what's going on that is all I've got that's an overview of link chain now Patricia is going to show why it's actually interesting for today and how you can use it with Minus TV [Applause] right hello everyone\n",
        "(13:20) like this yeah right so um yeah uh thank you all for being here um pretty exciting so I'll be focusing on how to build a domain expert agent uh essentially marrying mycb with Lang chain and we'll be taking a closer look at how the integration actually works so how we did it what are some next steps in terms of covering all the the vast round of work that Lang Chang has available as tools for for devs right so um I think we can start by looking at the ml engine abstraction in mysdb this is one of the main\n",
        "(14:02) abstractions we've developed in the in the recent past it's enabled us to essentially integrate over I think by this point to 10 mL Frameworks it includes our own automl engine Lightwood but it's there's a lot of other Integrations going on next level four time series um direct access to the open AI API and then there's Lang change so we essentially have exposed it as an additional ml engine with a Twist which is the the interesting bit that we'll discuss here so you can think of an ml\n",
        "(14:31) engine just basically as a template a class with a series of high-level methods of which probably the most important ones are create and predict we operate at the data frame level so it's really easy for a new contributor to just take their own framework for ML or even Library if it's something more specific figure out the mapping and then if you install those dependencies you can have it running essentially as an AI table abstraction where you've got your ml models and they behave as though it were data of course with additional\n",
        "(15:01) latencies when the models are more expensive to run but that's sort of the the the abstraction here now um the main components that we're using for line chain as of now again understanding this is a huge set of um like potential let's say Lang chain Stacks right what we're doing as of today is um essentially using a react based agent now react is a very cool paper um from people at Google and Princeton and essentially they figured out a way to um perhaps constrain the the action space of uh of a large language model\n",
        "(15:38) and structure it in a way that it acts it absurds and it thinks and doing this you get you gain access to tools that are an interface to the external World um and it's sort of a perfect blend of reasoning plus action and this is really powerful and maybe I'm mistaken but this is one of the first moments where line chain was like viral where people had this realization that oh actually large language models if you know prompted this particular way they're very powerful like we can do a lot of useful\n",
        "(16:11) stuff with the right tools so in the example down here you can see um you know there's a tool called think and then it actually reasons about a whole series of steps where the mission is to to like essentially uh put some object within a a big room right and it's going step by step which is a a famous another paper that's used as a reference here um and it reasons it's a pretty long chain so uh what we're doing in the minds to be um integration with langchang we're using a react-based agent which is a\n",
        "(16:45) quite a complex Beast as you can see from the example and we're augmenting it augmenting it with a conversational summary memory in case that the um let's say the token the tokens in the in the thought process Overflow what the receptive field of that model is and we're providing some external tools by external I mean outside of mysdb and then some internal tools which is like the in my opinion the novel bit of the integration so for external we have Google search Wikipedia um some other useful web pages\n",
        "(17:18) python Rebel which is a I'm pretty sure we'll discuss this at some point today but it's a double-edged sword you need to be very careful with that and then the uh the cool thing is we actually have um given access to the mysdb executor so what I mean by this is we have a stack of like SQL um like a SQL pipeline so we parse things we plan queries internally depending on the data source that you're using so that it's possible perhaps to I don't know like if you have three different data sources and you want to\n",
        "(17:48) have a huge like a a single view from which you will train your machine learning models in mindv um we have a way to deal with that right and finally the executor is what actually executes those steps in the plan um marrying all the data sources and the Machine learning models so launching has access to this and that's pretty exciting because it's essentially a way to automate a lot of the reasoning that people may do in a first pass when using mysdb right so um you can fetch metadata you can read levels you can write two tables\n",
        "(18:20) depending on the on the permissions you grant in the agent and um yeah it's just it's literally as if you were automating say a mindsdb developer that would connect some data source with some machine learning engine right so that's very interesting um some examples that are from mindstv itself rather than Lang chain on its own in this case you're just like um fetching the um the wiki quotes page on Albert Einstein getting some quotes with a reference back to user um you can see here in the in the first\n",
        "(18:53) blog here we have the the SQL editor from mycb we're just calling a link chain model we'll see in a bit how to create one but essentially you're giving an input like a synthetic input a single row so to speak in the tabular world um and just giving a budget of five iterations this of course has a direct impact on the cost of the of the call to the large language model but it essentially so the the takeaway here is that it goes it uses Wikipedia it uses Python and then it gets back the completion to the user it can get a bit\n",
        "(19:22) more complex so if you ask something that actually involves maths it would go use Python for multiplying the numbers then actually as we know large language models depending on which one you use can be great at code completion as well so it actually goes it writes uh an is prime function it evaluates the function with some with with the right number right and it says well it's not prime which is the correct answer by the way um so that leads me to a very short demo I think we're right on time um for a Dungeons and Dragons dungeon\n",
        "(19:53) master assistant now um we have a few gigs of Dungeons and Dragons in the team um we know it's a Time sync to prepare a campaign especially if you have to reference like cross reference different sources of Truth in terms of you know what monsters to use in your campaign what places you want to visit Etc so the demo here is just um it's not a chatbot per se it's more an assistant that goes looks at your data living in a in a database that's connected to my TV and uh then uses line chain to operate over\n",
        "(20:22) it as Harrison showed like a sort of a SQL database agent only that he uses the mysdb executor right so that's like the plot twist here let's see if we can get it to work um right so I have a live connection to my ZB here and the demo is around here somewhere here we go so I won't go into the details of connecting the the data source it's as a user you would just connect your database no surprises there and we have a table that has the summary of so it's from kaggle in this case but it's essentially\n",
        "(21:01) each monster name has a some set of attributes that is useful for a dungeon master stuff like how fast it moves intelligence wisdom Health points and so on and challenge reading also a very important one and then in terms of creating the model which is what we missed on the slides um you're essentially creating a mysdb model as any other predicting for a column that doesn't exist in the data set you're not so something we're missing here is the from Clause that you would typically use when you're training\n",
        "(21:33) from a from a data set let's say a regression problem or a classification problem in this case we're just fabricating or synthesizing the answer so we just say We'll predict something and it's going to be called recommendation and the using Clause sort of parameter parameterized it will parameterize the model in this case using the Lang chain engine so it could be you have a lot of options here but of course we want to use LINE chain using gpt4 and we will have a simple prompt template for now just for the sake of\n",
        "(22:04) example um because we can override this at prediction time which helps with the agility of the developer when figuring out maybe the prompt is not optimal well you can just go and correct that at prediction time and in in terms of tools we're actually just passing an empty list here we don't want to give external access to this particular agent so we won't give anything apart from the mysdb read access to the executor and of course verbose because we want to see what's happening under the hood here so\n",
        "(22:36) while the query runs because again reasoning can be expensive in terms of runtime and also dollars um we'll take a look at the query so we're just passing an input here which is what the dungeon master wants to check out and again you can do both predictions so if you instead use a join Clause here and passed a table with a bunch of different requests this is going to do a prediction with a batch instead of just a single one where is perfect for just poking around and seeing how the model behaves so we're\n",
        "(23:07) creating a recommender in the previous step and we're using it here with a bigger budget of iterations 20 in this case and here we're overriding the prompt template to be something that um of course there's a bit of a prompt gradient descent here but we are at a let's say an optimal prompt through trial and error where this thing actually works very well so we're doing a few things on the top half of this prompt where we set the stage it's a Dungeons and Dragons campaign assistant um you want to help recommend the best\n",
        "(23:38) monsters in the campaign given what the DM is asking for um and then a few best let's say best practices when operating with the mycb stack so always remember to check the metadata before poking around the table so that you know what columns there are what data types are involved check the value values with a distinct query limit your Clauses just in case to avoid overflowing the the token buffer um and then if you get syntax errors because you're using something that's way too sophisticated or perhaps it\n",
        "(24:09) comes from some particular database flavor then please refer to something more simple because at the moment anyway it might be just operates on a subset of SQL right and finally we inject vm's request here so this thing executed I'll quickly scroll over to the final answer and just show you that for the question of the DM which was I have a tropical jungle campaign with a group of seven and level seven and level eight adventurers what are some moderately challenging monsters and it provides a few recommendations\n",
        "(24:42) some of these may be um what you would expect in a jungle I guess so ape elephant perhaps um and they have levels of challenge sorry like challenge ratings that in my opinion are decent challenges so it's like a fine answer right and in terms of reasoning if we go back to the to the chain here that gets executed you can see that um maybe as opposed to Harrison's examples we're using the the minds to be tools so like the metadata to get the the information about the mariadb monsters table first which is provided by the executor of my\n",
        "(25:20) ZB now that it knows the columns it knows what to query for with a distinct um uh query and then again it sort of fails to provide the right format in the react framing so it's missing like it has to say action before actually doing the action right so um it tries again with that um fix in place which is done automatically by Lang chain which is awesome and then um just figuring out what monster types there are out there picking a few from this like summarized answer to something that would be appropriate for a jungle\n",
        "(25:53) setting and then just essentially like arbitrarily setting a range of challenge uh ratings that would be appropriate for what the DM wants finally getting a few rows out of that like out of the SQL query that it compiles and then just formatting the final answer that appears in the SQL editor so this is pretty interesting like you can you can take this thing you can run it as an automated job my ZB has the concept of jobs that run at some frequency you can also hook it up with apis I think there's a customer later on\n",
        "(26:25) that's showing how to do that um so you can also like deploy this in chat based sort of contexts and that opens up a lot of doors that were just recently like getting to explore in more depth um I'm personally very excited to figure out the the role of Lang chain as a sort of an automl orchestrator when you have access to different Frameworks different data sources how does that work really um but yeah that's the demo I think we're right on time for a set of q a if anyone has any questions but\n",
        "(26:58) um yeah thank you for for the patients here I guess one last thing would be in terms of future work I forgot to to mention this we know from Harrison's talk there's a lot of ground covered by launching our own implementation right now is very um I don't want to say basic but it went directly towards the place where we thought there would be most impact having said that we can improve the coverage of supported LMS types of Agents types of memories right um even some components we don't use at all so\n",
        "(27:31) like text splitter summarizers are things we're not currently using agent robustness is a huge one prompt templating is pretty much an art right now and we want to sort of elegantly deploy best practices for interacting with mindstv through line chain and um and yeah finally there should be an easy way for us to provide custom logic as new tools which is really powerful as well in terms of just opening up the doors to these large sandwich models that we know are so awesome so yeah thank you very much I'm opening it up\n",
        "(28:00) now for Q a so [Applause] hello so we can make it available easily there's a few fixes that I may have pushed yesterday but they will make its way into the repo really soon and at that point you can fully replicate this with the three commands that I posted so I'll do that after the talk so that you can poke around um oh there's a lot there uh okay how to get started with Lane chain so so we have um we have we have Python and typescript packages both of them have uh basically to get started\n",
        "(29:06) with link chain I would probably suggest you figure out an application that's interesting to you that the beauty of the large language models is they're so good at so many different things there's there's got to be something that's uh uh pretty interesting to you we have a lot of um and then in the link chain documentation we have a use cases section and so I would find kind of like the use case that's uh most most similar to that thing a lot of the use cases you you get started with in like five lines\n",
        "(29:30) of code or something like that and you have like a chat bot up and running or something like that um so I would get started there and then from there I would dive into um you know all the stuff that Patricio talked about I actually think what you added was like very advanced in terms of like it was a completely new prompt template there was a lot of parameters that you were passing and so I think like I would definitely start high level at a use case there's a lot of ways to get started and like five lines of code\n",
        "(29:52) and then you you know there's lots of there's lots of rabbit holes that you can uh go down um I completely forget the next two questions one of the best practice what are the best practices okay cool yeah the key features of LinkedIn I think um a lot of them were highlighted in kind of like the the the modules and components part um so so part of what Lane chain you know right now is this exciting New Era and building with L alums where you know we've been building with them most people have been building with them for\n",
        "(30:29) eight months maybe maybe a year and so no one like really knows a hundred percent what the best practices are but we've tried to crystallize what we think the most like common and good abstractions are into these like components um and so those are kind of like the key pieces and the key piece like in going you know an order of priority I'd say like you know the language model is kind of like the most key piece and so we've got kind of like a component there really focused on that and then there's\n",
        "(30:53) kind of like language model input and output so like the prompt as the input the output parser as the output um and then on the other end there's a lot of things related to data because I think a lot of the applications that we help enable are when you connect language models to data and so there's a lot of um there's a whole there's a whole section on on the documentation around document loaders like pre-processing text Splitters um uh uh uh uh embeddings Vector stores retrievers things like that and then the\n",
        "(31:23) third area of what I'd say there's a lot of components um in in Lane chain are that the agent and the reasoning stuff that Patricia showed off so this idea of using a language model to recover from errors as he showed off and and to decide what to do next and so I think like yeah there's basically like uh models and models input and output stuff there's data and and data and connecting that to language models and then there's the agent stuff and all the tools that agent agents can use as well so I think we have like 50\n",
        "(31:53) different tools ranging from like power bi to like uh hugging face models that you can call as tools to do like image generation and stuff like that um best practices for building with Lane chain um I think I I mean I when I build with link chain I I think what I like about LinkedIn is that it's I think it's pretty composable and and modular and so there's lots of different components and so like I think it's very natural to start high level with a chain and then as you go deep you're going to realize\n",
        "(32:23) that oh I need to change this prompt and so we should have for most chains like pretty easy mechanisms for passing in custom prompts um and and then if you need to actually change some of the logic inside the chain the the inner workings of the chain are just like you know other chains for the most part that are called and so you know you can you can kind of like copy that code bring it out and then there's still these these building blocks of link chain but you're now just creating your own custom chain and so I\n",
        "(32:47) think um yeah in terms of best practices I would say start high level don't be afraid to dive deep and then a lot of it I hope is enabled at composability so hopefully that should be pretty easy I'm going to go back to the audience we have a queue of questions on the live stream but anybody here have questions yeah go ahead hi how would you connect uh your own models with longchin if they're not like on open source or on hugging space or other places yeah so so um this is a great question which is\n",
        "(33:24) basically how to yeah how to connect and I'm going to extend this even more to how to connect maybe your um home like vector store or your home retriever or anything like that so um a lot of uh basically the Lang chain abstraction just an interface that needs to be implemented and so you can kind of like subclass a class and then Implement a function so for language models in particular you can subclass kind of like the the base llm class and then Implement a method which um I think there's a simple one that\n",
        "(33:53) takes takes in a string and returns a string and you basically just put whatever references your own models in there and then you can pass that around and use that to configure all the other chains like you would a normal model that exists okay I'll try it out thank you okay going quickly back to the live stream um how is mindstv helping ship Lang chain projects into production sure yeah so I think the main benefit potentially is like um the fact that you don't have to hop around like there's a single let's say place where your data\n",
        "(34:26) lives already it doesn't necessarily make a lot of sense to to um you know set up a separate Silo for this instead you can sort of go through mines to be connect the data source immediately even if it's actually multiple data sources you can also do that and then just get something running up very fast and when it's time to production like literally the model's already have SQL table so it's like it should be hopefully zero fiction um so yeah I think that's probably the best like the biggest benefit\n",
        "(34:55) um not sure if yeah I think that answers the questions like the main the main yeah benefit I think that's good um we have a bunch of the same question which is will the repo get uh posted online is the repo available it will be available yeah we'll make a separate uh repo for you guys to like clone and just play around with mines to be on the side yeah yeah so no worries there we'll get to that any any more questions foreign models and libraries like Lang chain are making it super easy to connect data\n",
        "(35:33) sources and query it what do you think will be the key differentiator between company and Company B doing the same thing is it just the quality of the prompts is it fine tuning where do you think like the Special Sauce or like the mode of companies trying to build apps uh will be yeah I I think that's a really good question I think um well one I don't think anyone really knows for sure like I think that's part of the excitement around like yeah it's it's a really it's an amazing time to be\n",
        "(36:07) building things my best guess to that answer is I think part of it definitely comes down to the data so there's definitely the data that like you as a company have and can Expose and I think um these types of interfaces make it really easy to expose that data to other folks without giving away the underlying like data itself I think there's also a separate thing which is um Gathering kind of like or seeing how users interact with that data and using that to improve your own prompts models or even data sources so what I mean by\n",
        "(36:38) that is um you know I think I think the obvious one is as you see people asking questions about your data and or interacting with your data let's say um and and if you start to gather feedback on that you can start to see like you can start to like a B test different prompts you can maybe start to use some of that data to fine-tune model I think it's to be clear I think those are still like things to Aspire to I don't think most people are doing that again because it's so early and so exploratory in a lot of things\n",
        "(37:06) um the other like interesting thing that I'll just say is like as you're doing this and this is something that I want to do more internally we have like chat Bots for the lane chain documentation or we work with companies that have chat Bots um those are those are full of insights right like what are people asking questions about oh that probably means our documentation isn't very good there or we should beef that up or something like that um and and so I think that's another way to just like yeah get this kind of like\n",
        "(37:28) data flywheel turning um and a lot of it comes back to like evaluating yeah like how good the model is doing the other thing I'll say as well is is just like there's I think another way to differentiate is like the depth of integration that you do so I think like at the end of the day if you're a single call to an llm there's not much defensibility there um if at the end of the day if you're doing like retrieval augmented generation from like a vector from like a single Vector store and you're just\n",
        "(37:54) doing like a single chain um you know honestly I don't know how much differentiation there is as well but if you look at like copilot they're like the way that they construct The Prompt is incredibly complex like they pull it from like four or five different sources of like information and and there there's this art now of like prompt engineering but I think like a better word for it that I like is like prompt construction like it comes down to how you construct The Prompt and and you know co-pilot does this extremely\n",
        "(38:20) well and extremely cleverly and it's it's it's not like uh it's not a single call to a language model it's a complex system when they're constructing this prompt and then passing it in and so I think there will be defensibility in adding more of these complex systems and integrating them more tightly with a bunch of different sources and getting and getting that kind of going may I just add a couple of things here I think one is uh just how well the model is aligned so when you see like agent\n",
        "(38:49) Behavior like like the one we showed off here um a lot of times the agent will let's say fall out of bounds and then it will be on Uncharted Territory where ideally you want it to have a specific way so there it becomes really important how how the elm provider actually trained the even the policy right usually with reinforcement learning on how the model behaves there and that's a huge I think a huge um potential differentiating Factor another one is that you're seeing companies start doing different types of\n",
        "(39:14) bets so for example anthropic I think is now trying out huge contexts where you have like a hundred thousand tokens or something where that becomes super super powerful if and again this is a research question that's open at the moment but um it's not entirely clear whether it's just better to do fine tuning or to just do in-context learning right so if you have a huge context you can fit a lot of examples um so there's I want to say it's a really just an open field right now we need to perhaps wait\n",
        "(39:41) a bit more until figuring out like apart from everything that Harrison said like yes what are the the key factors yeah well well if I can add to that you reminded me of another thing yeah as well which is to the point around like the agent and it be like agents are still like you saw it mess up twice in that one demo right like they're still not um there's they're still not kind of like they're still not off the shelf uh fully reliable and so like to be specific by remember that that demo is using like a zero shot prompt so we kind\n",
        "(40:10) of just like tell it what to do the main way that I've seen kind of like getting agents to be reliable is by providing like a few shot examples and the main way that you do this is kind of like collecting these examples you don't start with them right like I think that's the beauty of these generative AI applications is you don't start with data now you start with an idea it's really easy to use these language models you can do it zero shot you can tell what to do um but then I think the number one way\n",
        "(40:33) to improve um these types of applications is start collecting these two shot examples and using them in your prompts to help guide it um in into what to do and so I think like you know I I think getting that like and this gets back again to like the evaluation part of it but getting that evaluation data flywheel turning is incredibly important for enabling these more complex applications okay I think we got time for one more there you go thanks so much just a quick question so um do you already support or do you plan to support in mind CB also\n",
        "(41:07) you know storing the embeddings directly as a as a column do you have an integration with PG Vector uh to enable this rack that uh Harrison was talking about yeah that's a great question um so we're actually finishing support I think likely today you'll see a demo about that like really close to the embedding concept um right now it's per integration but we want to unify it into like a more standard interface so that people can do the let's say the vector DB operations like from the first step without having\n",
        "(41:34) to to delve into the details of a particular ml engine right so because we're seeing patterns it's fairly established how they work now we want to try to make that simple so that you can get going with Vector DBS even faster so the answer is yes and there's like multiple fronts of work that we're trying to explore actively yeah cool excited about that yeah for sure is that you don't only have text Data you also have you know numerical data categorical data and might be to solve all the problems that it needs to solve\n",
        "(42:08) it needs to create an embedding for each of the The Columns that that you have and and therefore it is really good at building a full Vector representation of a row so um you can solve yeah like semantic search that you will do traditionally for uh just when your your table only contains text but you can do search for recommendation engines uh where you're finding the nearest neighbors uh given the full Vector store of or and the full Vector representation of the row so I I think that that's when when it starts to become really\n",
        "(42:45) interesting where you can combine Text Plus other type of uh columns column information that is not just text and and use the vector store databases to solve problems that before were really hard to solve such as recommendation engines Etc okay thank you very much guys that was great [Music] thank you [Music] foreign    \n",
        "\"\"\"\n",
        "\n",
        "prompt = \"Summarize the following transcript in bullet points:\\n\" + transcript\n",
        "generated_text = llm_chain.run(prompt)\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09358c12124f4a17b1a234991417255c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f5425ca85374bd0b0f505e9d5fa3d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e382aa10c1b4365b58e26415973a324",
            "placeholder": "​",
            "style": "IPY_MODEL_48f53d1afc1049dda7ed1f31ba0a6246",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2e382aa10c1b4365b58e26415973a324": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f53d1afc1049dda7ed1f31ba0a6246": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63705f9227f741f4a2008eb7e66fe357": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b6d06203644a0a8aee02b37622331e",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09358c12124f4a17b1a234991417255c",
            "value": 7
          }
        },
        "6fbc58cb77cf4d1a82e9f28cc95ffaaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d724ed818b428185c4fc5db50bdb90",
            "placeholder": "​",
            "style": "IPY_MODEL_9cb38479c5814a189182206fce98e859",
            "value": " 7/7 [00:49&lt;00:00,  5.76s/it]"
          }
        },
        "82d724ed818b428185c4fc5db50bdb90": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b6d06203644a0a8aee02b37622331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cb38479c5814a189182206fce98e859": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fc95d12b7a44798bfbfbd3abbc12caf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e519161298c1403083cc8668064abcf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f5425ca85374bd0b0f505e9d5fa3d96",
              "IPY_MODEL_63705f9227f741f4a2008eb7e66fe357",
              "IPY_MODEL_6fbc58cb77cf4d1a82e9f28cc95ffaaa"
            ],
            "layout": "IPY_MODEL_9fc95d12b7a44798bfbfbd3abbc12caf"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
