{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU petals xformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "auth_token = os.environ[\"HUGGINGFACE_API_KEY\"]\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/media/limcheekin/My Passport/transformers_cache\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/media/limcheekin/My Passport/langchain-playground/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Jul 25 15:52:02.813 [\u001b[1m\u001b[34mINFO\u001b[0m] Make sure you follow the LLaMA's terms of use: https://bit.ly/llama2-license for LLaMA 2, https://bit.ly/llama-license for LLaMA 1\n",
      "Jul 25 15:52:02.817 [\u001b[1m\u001b[34mINFO\u001b[0m] Using DHT prefix: Llama-2-70b-chat-hf\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:32<00:00, 10.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.3 s, sys: 2.57 s, total: 5.86 s\n",
      "Wall time: 42.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from petals import AutoDistributedModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, use_auth_token=auth_token)\n",
    "# [WARN] Running the client with dtype bfloat16 on CPU may be slow, \n",
    "# since your CPU doesn't support AVX512. \n",
    "# Consider loading the model with torch_dtype='float32'\n",
    "# REF: https://github.com/bigscience-workshop/petals/issues/321\n",
    "model = AutoDistributedModelForCausalLM.from_pretrained(\n",
    "    model_name, use_auth_token=auth_token, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF: https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat/blob/main/model.py#L24\n",
    "DEFAULT_SYSTEM_PROMPT = \"You are a helpful assistant.\\nYou will try to answer user questions, but don't make up the answer if you don't have the answer.\\nYou will complete tasks by following user instructions.\"\n",
    "def get_prompt(message: str, chat_history: list[tuple[str, str]] = [],\n",
    "               system_prompt: str = DEFAULT_SYSTEM_PROMPT) -> str:\n",
    "    texts = [f'[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n']\n",
    "    for user_input, response in chat_history:\n",
    "        texts.append(f'{user_input.strip()} [/INST] {response.strip()} </s><s> [INST] ')\n",
    "    texts.append(f'{message.strip()} [/INST]')\n",
    "    return ''.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prompt(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Jul 25 15:54:29.250 [\u001b[1m\u001b[34mINFO\u001b[0m] Route found: 0:40 via …uCst2v => 40:80 via …o6L7CF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "You are a helpful assistant.\n",
      "You will try to answer user questions, but don't make up the answer if you don't have the answer.\n",
      "You will complete tasks by following user instructions.\n",
      "<</SYS>>\n",
      "\n",
      "What is AI? [/INST]  AI stands for Artificial Intelligence. It refers to the ability of machines or computer programs to mimic intelligent human behavior, such as learning, problem-solving, and decision-making. AI systems use algorithms and data to make predictions, classify objects, and generate insights,\n"
     ]
    }
   ],
   "source": [
    "# default torch.dtype='bfloat16', time=2m 21.6s\n",
    "# torch_dtype=torch.float32, time=1m 29.3\n",
    "# (torch.float32 faster but use 2x memory according to:\n",
    "#  https://github.com/bigscience-workshop/petals/issues/321)\n",
    "\n",
    "prompt = \"What is AI?\"\n",
    "inputs = tokenizer(get_prompt(prompt), return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model.generate(inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generate_text = pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
